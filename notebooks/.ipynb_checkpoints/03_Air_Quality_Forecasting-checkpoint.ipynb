{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6772e8-f710-480f-a40d-358c8a137f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Predicción de Calidad del Aire (PM2.5)\n",
    "# \n",
    "# Este notebook entrena y evalúa modelos para predecir la concentración de PM2.5.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Importar funciones del pipeline\n",
    "from ts_weather_pipeline.api import fetch_open_meteo_archive, fetch_air_quality_data\n",
    "from ts_weather_pipeline.preprocessing import fill_and_resample, add_time_features, add_lags, add_fourier\n",
    "from ts_weather_pipeline.baselines import naive_forecast, fit_var, fit_arima\n",
    "from ts_weather_pipeline.deep_learning import make_supervised, build_lstm_model, build_transformer_encoder\n",
    "from ts_weather_pipeline.evaluation import evaluate_multi_horizon\n",
    "from ts_weather_pipeline.transformer import make_supervised_transformer\n",
    "\n",
    "# Configuración\n",
    "target = \"pm2_5\"  # Cambiamos el target a PM2.5\n",
    "plt.style.use('default')\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# %%\n",
    "# Cargar datos\n",
    "try:\n",
    "    df = pd.read_csv('../data/air_quality_data.csv', index_col=0, parse_dates=True)\n",
    "    print(\"Datos de calidad del aire cargados desde archivo CSV\")\n",
    "except:\n",
    "    print(\"Descargando datos...\")\n",
    "    # Primero obtener datos meteorológicos\n",
    "    meteo_df = fetch_open_meteo_archive(\n",
    "        lat=-34.6037, \n",
    "        lon=-58.3816, \n",
    "        start_date=\"2020-01-01\",\n",
    "        end_date=\"2025-09-10\",\n",
    "        hourly_vars=[\"temperature_2m\", \"relativehumidity_2m\", \"windspeed_10m\", \"precipitation\", \"pressure_msl\"]\n",
    "    )\n",
    "    \n",
    "    # Luego obtener datos de calidad del aire (esto es un ejemplo, necesitarías implementar fetch_air_quality_data)\n",
    "    # aq_df = fetch_air_quality_data(lat=-34.6037, lon=-58.3816, start_date=\"2020-01-01\", end_date=\"2025-09-10\")\n",
    "    \n",
    "    # Por ahora, usaremos datos sintéticos para PM2.5 como ejemplo\n",
    "    # En la práctica, necesitarías una fuente real de datos de calidad del aire\n",
    "    np.random.seed(42)\n",
    "    pm25_values = np.random.lognormal(mean=2.5, sigma=0.8, size=len(meteo_df))\n",
    "    pm25_values = np.clip(pm25_values, 5, 150)  # Valores razonables para PM2.5\n",
    "    \n",
    "    # Crear DataFrame con datos sintéticos\n",
    "    aq_df = pd.DataFrame({\n",
    "        'pm2_5': pm25_values,\n",
    "        'no2': np.random.lognormal(mean=2.0, sigma=0.7, size=len(meteo_df)),\n",
    "        'o3': np.random.lognormal(mean=3.0, sigma=0.5, size=len(meteo_df))\n",
    "    }, index=meteo_df.index)\n",
    "    \n",
    "    # Combinar datos meteorológicos y de calidad del aire\n",
    "    df = pd.concat([meteo_df, aq_df], axis=1)\n",
    "    df = fill_and_resample(df, freq=\"H\")\n",
    "    \n",
    "    # Guardar datos combinados\n",
    "    df.to_csv('../data/air_quality_data.csv')\n",
    "    print(\"Datos descargados y guardados en CSV\")\n",
    "\n",
    "print(f\"Datos: {df.shape}\")\n",
    "print(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
    "\n",
    "# %%\n",
    "# Añadir características\n",
    "df = add_time_features(df)\n",
    "df = add_lags(df, target, lags=(1, 24, 168))  # 1h, 24h, 1 semana\n",
    "df = add_fourier(df, period_hours=24, n_harmonics=3)\n",
    "\n",
    "# Añadir lags de variables meteorológicas que pueden afectar la calidad del aire\n",
    "for var in [\"temperature_2m\", \"relativehumidity_2m\", \"windspeed_10m\", \"precipitation\"]:\n",
    "    df = add_lags(df, var, lags=(1, 24), prefix=f\"{var}_lag\")\n",
    "\n",
    "# Eliminar valores nulos\n",
    "df = df.dropna()\n",
    "print(f\"Datos después de preprocesamiento: {df.shape}\")\n",
    "\n",
    "# %%\n",
    "# Dividir datos\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.15)\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "\n",
    "# %%\n",
    "# Escalar datos - usar StandardScaler para PM2.5 ya que puede tener distribución log-normal\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "features = [col for col in train_df.select_dtypes(include=\"number\").columns if col != target]\n",
    "target_data = train_df[target].values.reshape(-1, 1)\n",
    "\n",
    "scaler_x.fit(train_df[features])\n",
    "scaler_y.fit(target_data)\n",
    "\n",
    "train_scaled = pd.DataFrame(scaler_x.transform(train_df[features]), \n",
    "                           columns=features, index=train_df.index)\n",
    "train_scaled[target] = scaler_y.transform(target_data).flatten()\n",
    "\n",
    "val_scaled = pd.DataFrame(scaler_x.transform(val_df[features]), \n",
    "                         columns=features, index=val_df.index)\n",
    "val_scaled[target] = scaler_y.transform(val_df[target].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "test_scaled = pd.DataFrame(scaler_x.transform(test_df[features]), \n",
    "                          columns=features, index=test_df.index)\n",
    "test_scaled[target] = scaler_y.transform(test_df[target].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Guardar scalers\n",
    "joblib.dump(scaler_x, '../models/air_quality_scaler_x.pkl')\n",
    "joblib.dump(scaler_y, '../models/air_quality_scaler_y.pkl')\n",
    "with open('../models/air_quality_features.json', 'w') as f:\n",
    "    json.dump({\"features\": features, \"target\": target}, f)\n",
    "\n",
    "# %%\n",
    "# Preparar datos supervisados\n",
    "input_width = 168  # 1 semana\n",
    "output_width = 24  # 24 horas\n",
    "\n",
    "X_train, y_train, _ = make_supervised(train_scaled, target, input_width, output_width)\n",
    "X_val, y_val, _ = make_supervised(val_scaled, target, input_width, output_width)\n",
    "X_test, y_test, _ = make_supervised(test_scaled, target, input_width, output_width)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# %%\n",
    "# Modelo Naive (baseline) - usar el último valor disponible\n",
    "naive_pred = naive_forecast(train_df[target], output_width)\n",
    "naive_eval = evaluate_multi_horizon(test_df[target].values[:output_width], naive_pred)\n",
    "\n",
    "print(\"Naive Model Evaluation:\")\n",
    "print(f\"MAE: {naive_eval['mae']:.2f}, RMSE: {naive_eval['rmse']:.2f}, MAPE: {naive_eval['mape']:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Modelo ARIMA\n",
    "print(\"Entrenando modelo ARIMA...\")\n",
    "try:\n",
    "    # Para calidad del aire, incluir variables exógenas relevantes\n",
    "    exog_feats = [\"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\", \n",
    "                 \"temperature_2m_lag1\", \"windspeed_10m_lag1\", \"relativehumidity_2m_lag1\"]\n",
    "    \n",
    "    arima_res = fit_arima(train_df[target], exog=train_df[exog_feats])\n",
    "    arima_fc = arima_res.get_forecast(steps=output_width, \n",
    "                                     exog=test_df[exog_feats].iloc[:output_width])\n",
    "    \n",
    "    arima_eval = evaluate_multi_horizon(test_df[target].iloc[:output_width].values, \n",
    "                                       arima_fc.predicted_mean.values)\n",
    "    \n",
    "    print(\"ARIMA Evaluation:\")\n",
    "    print(f\"MAE: {arima_eval['mae']:.2f}, RMSE: {arima_eval['rmse']:.2f}, MAPE: {arima_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo ARIMA\n",
    "    joblib.dump(arima_res, '../models/arima_air_quality_model.pkl')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ARIMA failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Modelo LSTM\n",
    "print(\"Entrenando modelo LSTM...\")\n",
    "try:\n",
    "    lstm_model = build_lstm_model(\n",
    "        (input_width, len(features)), \n",
    "        output_width, \n",
    "        units=64,  # Aumentado para capturar patrones más complejos\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=30,  # Aumentado para mejor convergencia\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluar LSTM\n",
    "    y_pred_lstm = lstm_model.predict(X_test, batch_size=32)\n",
    "    lstm_eval = evaluate_multi_horizon(y_test, y_pred_lstm)\n",
    "    \n",
    "    print(\"LSTM Evaluation:\")\n",
    "    print(f\"MAE: {lstm_eval['mae']:.2f}, RMSE: {lstm_eval['rmse']:.2f}, MAPE: {lstm_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo LSTM\n",
    "    lstm_model.save('../models/lstm_air_quality_model.h5')\n",
    "    \n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('MAE durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LSTM failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Modelo Transformer\n",
    "print(\"Entrenando modelo Transformer...\")\n",
    "try:\n",
    "    # Preparar datos para Transformer\n",
    "    X_train_tf, y_train_tf, _ = make_supervised_transformer(train_scaled, target, input_width, output_width)\n",
    "    X_val_tf, y_val_tf, _ = make_supervised_transformer(val_scaled, target, input_width, output_width)\n",
    "    X_test_tf, y_test_tf, _ = make_supervised_transformer(test_scaled, target, input_width, output_width)\n",
    "    \n",
    "    transformer_model = build_transformer_encoder(\n",
    "        (input_width, len(features)),\n",
    "        output_width,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        ff_dim=256,  # Aumentado para mayor capacidad\n",
    "        num_blocks=3,  # Aumentado para mayor profundidad\n",
    "        dropout=0.1,\n",
    "        lr=1e-4  # Reducido para mejor convergencia\n",
    "    )\n",
    "    \n",
    "    history_tf = transformer_model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        validation_data=(X_val_tf, y_val_tf),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluar Transformer\n",
    "    y_pred_tf = transformer_model.predict(X_test_tf, batch_size=32)\n",
    "    tf_eval = evaluate_multi_horizon(y_test_tf, y_pred_tf)\n",
    "    \n",
    "    print(\"Transformer Evaluation:\")\n",
    "    print(f\"MAE: {tf_eval['mae']:.2f}, RMSE: {tf_eval['rmse']:.2f}, MAPE: {tf_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo Transformer\n",
    "    transformer_model.save('../models/transformer_air_quality_model.h5')\n",
    "    \n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_tf.history['loss'], label='Training Loss')\n",
    "    plt.plot(history_tf.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_tf.history['mean_absolute_error'], label='Training MAE')\n",
    "    plt.plot(history_tf.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('MAE durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Transformer failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Comparar todos los modelos\n",
    "results = {\n",
    "    'Naive': naive_eval,\n",
    "    'ARIMA': arima_eval if 'arima_eval' in locals() else None,\n",
    "    'LSTM': lstm_eval if 'lstm_eval' in locals() else None,\n",
    "    'Transformer': tf_eval if 'tf_eval' in locals() else None\n",
    "}\n",
    "\n",
    "# Crear gráfico de comparación\n",
    "models = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        models.append(model_name)\n",
    "        mae_scores.append(result['mae'])\n",
    "        rmse_scores.append(result['rmse'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, mae_scores, width, label='MAE')\n",
    "plt.bar(x + width/2, rmse_scores, width, label='RMSE')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Comparación de Modelos de Predicción de Calidad del Aire (PM2.5)')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Visualizar predicciones (desescaladas)\n",
    "y_test_descaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "if 'y_pred_lstm' in locals():\n",
    "    y_pred_lstm_descaled = scaler_y.inverse_transform(y_pred_lstm.reshape(-1, 1)).reshape(y_pred_lstm.shape)\n",
    "\n",
    "if 'y_pred_tf' in locals():\n",
    "    y_pred_tf_descaled = scaler_y.inverse_transform(y_pred_tf.reshape(-1, 1)).reshape(y_pred_tf.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 1: Naive\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(test_df[target].values[:output_width], label='Real')\n",
    "plt.plot(naive_pred, label='Naive')\n",
    "plt.title('Predicción Naive')\n",
    "plt.legend()\n",
    "plt.ylabel('PM2.5 (μg/m³)')\n",
    "\n",
    "# Gráfico 2: ARIMA\n",
    "if 'arima_fc' in locals():\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(test_df[target].values[:output_width], label='Real')\n",
    "    plt.plot(arima_fc.predicted_mean.values, label='ARIMA')\n",
    "    plt.title('Predicción ARIMA')\n",
    "    plt.legend()\n",
    "    plt.ylabel('PM2.5 (μg/m³)')\n",
    "\n",
    "# Gráfico 3: LSTM\n",
    "if 'y_pred_lstm' in locals():\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(y_test_descaled[0], label='Real')\n",
    "    plt.plot(y_pred_lstm_descaled[0], label='LSTM')\n",
    "    plt.title('Predicción LSTM')\n",
    "    plt.legend()\n",
    "    plt.ylabel('PM2.5 (μg/m³)')\n",
    "\n",
    "# Gráfico 4: Transformer\n",
    "if 'y_pred_tf' in locals():\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(y_test_descaled[0], label='Real')\n",
    "    plt.plot(y_pred_tf_descaled[0], label='Transformer')\n",
    "    plt.title('Predicción Transformer')\n",
    "    plt.legend()\n",
    "    plt.ylabel('PM2.5 (μg/m³)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Guardar resultados de evaluación\n",
    "evaluation_results = {\n",
    "    'naive': naive_eval,\n",
    "    'arima': arima_eval if 'arima_eval' in locals() else None,\n",
    "    'lstm': lstm_eval if 'lstm_eval' in locals() else None,\n",
    "    'transformer': tf_eval if 'tf_eval' in locals() else None\n",
    "}\n",
    "\n",
    "with open('../models/air_quality_evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "print(\"Resultados de evaluación guardados en models/air_quality_evaluation_results.json\")\n",
    "\n",
    "# %%\n",
    "# Análisis de importancia de variables (solo para modelos que lo permiten)\n",
    "# Para LSTM y Transformer, se pueden usar técnicas como SHAP o permutación\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Ejemplo con LSTM (usando solo el primer batch para no sobrecargar)\n",
    "    X_sample = X_test[:10]\n",
    "    y_sample = y_test[:10]\n",
    "    \n",
    "    # Función para calcular importancia por permutación\n",
    "    def model_predict(X):\n",
    "        return lstm_model.predict(X, verbose=0)\n",
    "    \n",
    "    # Calcular importancia\n",
    "    perm_importance = permutation_importance(\n",
    "        model_predict, X_sample.reshape(X_sample.shape[0], -1), y_sample,\n",
    "        n_repeats=10, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Obtener características importantes\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features * input_width,  # Repetir para cada paso de tiempo\n",
    "        'importance': perm_importance.importances_mean\n",
    "    })\n",
    "    \n",
    "    # Agrupar por característica (promedio a través del tiempo)\n",
    "    grouped_importance = feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    grouped_importance.head(15).plot(kind='barh')\n",
    "    plt.title('Importancia de Variables para Predicción de PM2.5')\n",
    "    plt.xlabel('Importancia (disminución en precisión)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Análisis de importancia falló: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
