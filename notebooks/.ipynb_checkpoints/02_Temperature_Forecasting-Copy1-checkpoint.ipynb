{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6772e8-f710-480f-a40d-358c8a137f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde archivo CSV\n",
      "Datos: (49920, 4)\n",
      "Datos después de preprocesamiento: (49752, 20)\n",
      "Train: 34826, Validation: 7462, Test: 7464\n",
      "X_train: (34635, 168, 20), y_train: (34635, 24)\n",
      "X_val: (7271, 168, 20), y_val: (7271, 24)\n",
      "X_test: (7273, 168, 20), y_test: (7273, 24)\n",
      "Naive Model Evaluation:\n",
      "MAE: 2.98, RMSE: 2.98, MAPE: 14.60%\n",
      "Entrenando modelo ARIMA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bita/.pyenv/versions/3.9.18/envs/ts_weather/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/bita/.pyenv/versions/3.9.18/envs/ts_weather/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/bita/.pyenv/versions/3.9.18/envs/ts_weather/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Evaluation:\n",
      "MAE: 2.45, RMSE: 2.45, MAPE: 11.89%\n",
      "ARIMA failed: [Errno 2] No such file or directory: 'models/arima_temperature_model.pkl'\n",
      "Entrenando modelo LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:04:28.568559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-09-20 23:04:28.569807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570472: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570608: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.570914: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.571053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bita/hadoop/lib/native:\n",
      "2025-09-20 23:04:28.571077: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-09-20 23:04:28.574543: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-20 23:04:30.502403: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 465494400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1083/1083 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.1186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:08:44.620802: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 97722240 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083/1083 [==============================] - 266s 239ms/step - loss: 0.0302 - mean_absolute_error: 0.1186 - val_loss: 0.0061 - val_mean_absolute_error: 0.0613\n",
      "Epoch 2/20\n",
      " 419/1083 [==========>...................] - ETA: 2:29 - loss: 0.0081 - mean_absolute_error: 0.0691"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Predicción de Temperatura\n",
    "# \n",
    "# Este notebook entrena y evalúa modelos para predecir temperatura.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Importar funciones del pipeline\n",
    "from ts_weather_pipeline.api import fetch_open_meteo_archive\n",
    "from ts_weather_pipeline.preprocessing import fill_and_resample, add_time_features, add_lags, add_fourier\n",
    "from ts_weather_pipeline.baselines import naive_forecast, fit_var, fit_arima\n",
    "from ts_weather_pipeline.deep_learning import make_supervised, build_lstm_model, build_transformer_encoder\n",
    "from ts_weather_pipeline.evaluation import evaluate_multi_horizon\n",
    "from ts_weather_pipeline.transformer import make_supervised_transformer\n",
    "\n",
    "# Configuración\n",
    "target = \"temperature_2m\"\n",
    "plt.style.use('default')\n",
    "\n",
    "# %%\n",
    "# Cargar datos\n",
    "try:\n",
    "    df = pd.read_csv('../data/temperature_data.csv', index_col=0, parse_dates=True)\n",
    "    print(\"Datos cargados desde archivo CSV\")\n",
    "except:\n",
    "    print(\"Descargando datos...\")\n",
    "    df = fetch_open_meteo_archive(\n",
    "        lat=-34.6037, \n",
    "        lon=-58.3816, \n",
    "        start_date=\"2020-01-01\",\n",
    "        end_date=\"2025-09-10\",\n",
    "        hourly_vars=[\"temperature_2m\", \"relativehumidity_2m\", \"windspeed_10m\", \"precipitation\"]\n",
    "    )\n",
    "    df = fill_and_resample(df, freq=\"H\")\n",
    "\n",
    "print(f\"Datos: {df.shape}\")\n",
    "\n",
    "# %%\n",
    "# Añadir características\n",
    "df = add_time_features(df)\n",
    "df = add_lags(df, target, lags=(1, 24, 168))  # 1h, 24h, 1 semana\n",
    "df = add_fourier(df, period_hours=24, n_harmonics=3)\n",
    "\n",
    "# Eliminar valores nulos\n",
    "df = df.dropna()\n",
    "print(f\"Datos después de preprocesamiento: {df.shape}\")\n",
    "\n",
    "# %%\n",
    "# Dividir datos\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.15)\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "\n",
    "# %%\n",
    "# Escalar datos\n",
    "scaler = MinMaxScaler()\n",
    "features = train_df.select_dtypes(include=\"number\").columns.tolist()\n",
    "scaler.fit(train_df[features])\n",
    "\n",
    "train_scaled = pd.DataFrame(scaler.transform(train_df[features]), \n",
    "                           columns=features, index=train_df.index)\n",
    "val_scaled = pd.DataFrame(scaler.transform(val_df[features]), \n",
    "                         columns=features, index=val_df.index)\n",
    "test_scaled = pd.DataFrame(scaler.transform(test_df[features]), \n",
    "                          columns=features, index=test_df.index)\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, '../models/temperature_scaler.pkl')\n",
    "with open('../models/temperature_features.json', 'w') as f:\n",
    "    json.dump({\"features\": features, \"target\": target}, f)\n",
    "\n",
    "# %%\n",
    "# Preparar datos supervisados\n",
    "input_width = 168  # 1 semana\n",
    "output_width = 24  # 24 horas\n",
    "\n",
    "X_train, y_train, _ = make_supervised(train_scaled, target, input_width, output_width)\n",
    "X_val, y_val, _ = make_supervised(val_scaled, target, input_width, output_width)\n",
    "X_test, y_test, _ = make_supervised(test_scaled, target, input_width, output_width)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# %%\n",
    "# Modelo Naive (baseline)\n",
    "naive_pred = naive_forecast(train_df[target], output_width)\n",
    "naive_eval = evaluate_multi_horizon(test_df[target].values[:output_width], naive_pred)\n",
    "\n",
    "print(\"Naive Model Evaluation:\")\n",
    "print(f\"MAE: {naive_eval['mae']:.2f}, RMSE: {naive_eval['rmse']:.2f}, MAPE: {naive_eval['mape']:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Modelo ARIMA\n",
    "print(\"Entrenando modelo ARIMA...\")\n",
    "try:\n",
    "    exog_feats = [\"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\"]\n",
    "    arima_res = fit_arima(train_df[target], exog=train_df[exog_feats])\n",
    "    arima_fc = arima_res.get_forecast(steps=output_width, \n",
    "                                     exog=test_df[exog_feats].iloc[:output_width])\n",
    "    \n",
    "    arima_eval = evaluate_multi_horizon(test_df[target].iloc[:output_width].values, \n",
    "                                       arima_fc.predicted_mean.values)\n",
    "    \n",
    "    print(\"ARIMA Evaluation:\")\n",
    "    print(f\"MAE: {arima_eval['mae']:.2f}, RMSE: {arima_eval['rmse']:.2f}, MAPE: {arima_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo ARIMA\n",
    "    joblib.dump(arima_res, '../models/arima_temperature_model.pkl')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ARIMA failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Modelo LSTM\n",
    "print(\"Entrenando modelo LSTM...\")\n",
    "try:\n",
    "    lstm_model = build_lstm_model(\n",
    "        (input_width, len(features)), \n",
    "        output_width, \n",
    "        units=32,  # Reducido para ahorrar memoria\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,  # Reducido para ahorrar tiempo\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar LSTM\n",
    "    y_pred_lstm = lstm_model.predict(X_test, batch_size=32)\n",
    "    lstm_eval = evaluate_multi_horizon(y_test, y_pred_lstm)\n",
    "    \n",
    "    print(\"LSTM Evaluation:\")\n",
    "    print(f\"MAE: {lstm_eval['mae']:.2f}, RMSE: {lstm_eval['rmse']:.2f}, MAPE: {lstm_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo LSTM\n",
    "    lstm_model.save('../models/lstm_temperature_model.h5')\n",
    "    \n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('MAE durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LSTM failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Modelo Transformer\n",
    "print(\"Entrenando modelo Transformer...\")\n",
    "try:\n",
    "    # Preparar datos para Transformer\n",
    "    X_train_tf, y_train_tf, _ = make_supervised_transformer(train_scaled, target, input_width, output_width)\n",
    "    X_val_tf, y_val_tf, _ = make_supervised_transformer(val_scaled, target, input_width, output_width)\n",
    "    X_test_tf, y_test_tf, _ = make_supervised_transformer(test_scaled, target, input_width, output_width)\n",
    "    \n",
    "    transformer_model = build_transformer_encoder(\n",
    "        (input_width, len(features)),\n",
    "        output_width,\n",
    "        d_model=64,  # Reducido para ahorrar memoria\n",
    "        num_heads=4,\n",
    "        ff_dim=128,\n",
    "        num_blocks=2,\n",
    "        dropout=0.1,\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    history_tf = transformer_model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        validation_data=(X_val_tf, y_val_tf),\n",
    "        epochs=20,  # Reducido para ahorrar tiempo\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar Transformer\n",
    "    y_pred_tf = transformer_model.predict(X_test_tf, batch_size=32)\n",
    "    tf_eval = evaluate_multi_horizon(y_test_tf, y_pred_tf)\n",
    "    \n",
    "    print(\"Transformer Evaluation:\")\n",
    "    print(f\"MAE: {tf_eval['mae']:.2f}, RMSE: {tf_eval['rmse']:.2f}, MAPE: {tf_eval['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar modelo Transformer\n",
    "    transformer_model.save('../models/transformer_temperature_model.h5')\n",
    "    \n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_tf.history['loss'], label='Training Loss')\n",
    "    plt.plot(history_tf.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_tf.history['mean_absolute_error'], label='Training MAE')\n",
    "    plt.plot(history_tf.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('MAE durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Transformer failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Comparar todos los modelos\n",
    "results = {\n",
    "    'Naive': naive_eval,\n",
    "    'ARIMA': arima_eval if 'arima_eval' in locals() else None,\n",
    "    'LSTM': lstm_eval if 'lstm_eval' in locals() else None,\n",
    "    'Transformer': tf_eval if 'tf_eval' in locals() else None\n",
    "}\n",
    "\n",
    "# Crear gráfico de comparación\n",
    "models = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        models.append(model_name)\n",
    "        mae_scores.append(result['mae'])\n",
    "        rmse_scores.append(result['rmse'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, mae_scores, width, label='MAE')\n",
    "plt.bar(x + width/2, rmse_scores, width, label='RMSE')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Comparación de Modelos de Predicción de Temperatura')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Visualizar predicciones\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 1: Naive\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(test_df[target].values[:output_width], label='Real')\n",
    "plt.plot(naive_pred, label='Naive')\n",
    "plt.title('Predicción Naive')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico 2: ARIMA\n",
    "if 'arima_fc' in locals():\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(test_df[target].values[:output_width], label='Real')\n",
    "    plt.plot(arima_fc.predicted_mean.values, label='ARIMA')\n",
    "    plt.title('Predicción ARIMA')\n",
    "    plt.legend()\n",
    "\n",
    "# Gráfico 3: LSTM\n",
    "if 'y_pred_lstm' in locals():\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(y_test[0], label='Real')\n",
    "    plt.plot(y_pred_lstm[0], label='LSTM')\n",
    "    plt.title('Predicción LSTM')\n",
    "    plt.legend()\n",
    "\n",
    "# Gráfico 4: Transformer\n",
    "if 'y_pred_tf' in locals():\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(y_test_tf[0], label='Real')\n",
    "    plt.plot(y_pred_tf[0], label='Transformer')\n",
    "    plt.title('Predicción Transformer')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Guardar resultados de evaluación\n",
    "evaluation_results = {\n",
    "    'naive': naive_eval,\n",
    "    'arima': arima_eval if 'arima_eval' in locals() else None,\n",
    "    'lstm': lstm_eval if 'lstm_eval' in locals() else None,\n",
    "    'transformer': tf_eval if 'tf_eval' in locals() else None\n",
    "}\n",
    "\n",
    "with open('../models/temperature_evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "print(\"Resultados de evaluación guardados en models/temperature_evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd26dc-cc3e-4050-a80f-4cf6a35de8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
